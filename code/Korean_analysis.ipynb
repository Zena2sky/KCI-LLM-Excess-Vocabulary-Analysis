{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac88699",
   "metadata": {
    "id": "fac88699"
   },
   "source": [
    "\n",
    "## KCI 사회과학 분야 한국어 초록 분석(2004–2024) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5749d19",
   "metadata": {
    "id": "e5749d19"
   },
   "source": [
    "# 1) 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e73c8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63e73c8b",
    "outputId": "fa016cd0-4fb1-41ac-e90f-cd30384afccb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv(r'/home/work/KCI_LLM_Lab/raw data/사회과학_korean_papers_2004-2024_20251002T195849.csv')  # 2004–2024년 전체 데이터\n",
    "\n",
    "# 불러온 데이터 정보 확인\n",
    "print(\"데이터 크기:\", df.shape)\n",
    "print(\"\\n컬럼 목록:\", list(df.columns))\n",
    "print(\"\\n데이터 미리보기 (앞 3줄):\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40aa797",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c40aa797",
    "outputId": "e279dd4c-7fba-4244-aaa3-9b5bab43bba9"
   },
   "outputs": [],
   "source": [
    "# 사용하고자 하는 컬럼 추출\n",
    "df = df[['논문ID', '발행년도', '논문제목', '초록', '주제분야']]\n",
    "print(f\"필터링된 데이터프레임 크기: {df.shape}\")\n",
    "\n",
    "# 결측치 확인\n",
    "print(\"\\n결측치 확인:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fe643",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "644fe643",
    "outputId": "44c1b373-70a5-4020-c172-677260207428"
   },
   "outputs": [],
   "source": [
    "# 공백/빈문자열 및 결측 문자열 → NA 처리\n",
    "for c in ['논문제목', '초록']:\n",
    "    s = df[c].astype(str).str.strip().str.lower()\n",
    "    # \"nan\", \"none\", \"null\", \"<na>\", \"n/a\" 같은 문자열 결측 토큰 제거\n",
    "    s = s.replace(r'^(nan|none|null|<na>|n/a)$', '', regex=True)\n",
    "    # 빈 문자열을 NA로 바꾸기\n",
    "    s = s.replace({'': pd.NA})\n",
    "    df[c] = s\n",
    "\n",
    "# 초록만 필수로 남기기\n",
    "df = df.dropna(subset=['초록'])\n",
    "print(f\"[OK] 초록 결측 제거 후 크기: {df.shape}\")\n",
    "\n",
    "# 데이터 정보 확인\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849f470",
   "metadata": {
    "id": "3849f470"
   },
   "source": [
    "### 추후 주제별 비교를 위해 '주제분야' 컬럼 결측 보정 및 계층 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12627b",
   "metadata": {
    "id": "fb12627b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 결측/유사결측 정의\n",
    "NULL_LIKE = {'', 'nan', 'none', 'null', 'na', '미기재', 'N/A'}\n",
    "\n",
    "# 문자열 표준화: 공백 정리, 구분자 ' > '로 통일\n",
    "s = df['주제분야'].astype(str).str.strip()\n",
    "\n",
    "# '인문학  >  역사학' 같은 경우 공백 정리 및 다양한 기호 통일\n",
    "s = s.replace(r'\\s*>\\s*', ' > ', regex=True)   # 구분자 좌우 공백 통일\n",
    "s = s.replace(r'\\s+', ' ', regex=True)         # 다중 공백 → 단일 공백\n",
    "\n",
    "# 유사결측을 NA로 치환\n",
    "s_lower = s.str.lower()\n",
    "s = s.where(~s_lower.isin({x.lower() for x in NULL_LIKE}), pd.NA)\n",
    "\n",
    "# 결측 채움(분석 편의용 라벨)\n",
    "s = s.fillna('미상')\n",
    "\n",
    "# 계층 분해: 대/중/소 분류 (최대 3단계로 가정함, 더 깊으면 마지막에 합침)\n",
    "# 단계가 많은 예시 : 인문학 > 한국어와문학 > 국어학 > 국어사\n",
    "parts = s.str.split(' > ', n=2, expand=True)  # n=2 ⇒ 최대 3조각\n",
    "parts.columns = ['대분류','중분류','소분류']\n",
    "\n",
    "# 소분류에 여전히 ' > '가 남았다면(4단계 이상) 마지막 조각에 이어붙여진 상태이므로 그대로 둡니다.\n",
    "\n",
    "# 정리된 컬럼 반영\n",
    "df['주제분야_표준'] = s\n",
    "df[['대분류','중분류','소분류']] = parts\n",
    "\n",
    "# 타입: 범주형으로 변환(메모리 절약 + 그룹화 속도)\n",
    "for c in ['주제분야_표준','대분류','중분류','소분류']:\n",
    "    df[c] = df[c].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1526c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f1526c6",
    "outputId": "a7033399-4df9-48c8-87b5-3e84ae1feaea"
   },
   "outputs": [],
   "source": [
    "# 결과 저장 경로 설정\n",
    "from pathlib import Path\n",
    "save_dir = Path('results_S_ver4')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. 연도별 데이터 개수\n",
    "year_counts = df['발행년도'].value_counts().sort_index()\n",
    "print(\"연도별 논문 수:\")\n",
    "print(year_counts)\n",
    "year_counts.to_csv(save_dir / 'SS_yearly_counts.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 2. 연도별 중분류 개수\n",
    "year_category_counts = df.groupby('발행년도')['중분류'].nunique()\n",
    "print(\"\\n연도별 중분류 개수:\")\n",
    "print(year_category_counts)\n",
    "year_category_counts.to_csv(save_dir / 'SS_yearly_category_counts.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 3. 중분류별 논문 수 집계\n",
    "category_stats = df['중분류'].value_counts()\n",
    "print(\"\\n중분류별 논문 수:\")\n",
    "print(category_stats)\n",
    "category_stats.to_csv(save_dir / 'SS_category_paper_counts.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 저장 완료 메시지\n",
    "print(\"\\n✅ 저장 완료:\")\n",
    "print(f\" - 연도별 논문 수: {save_dir}/SS_yearly_counts.csv\")\n",
    "print(f\" - 연도별 중분류 수: {save_dir}/SS_yearly_category_counts.csv\")\n",
    "print(f\" - 중분류별 논문 수: {save_dir}/SS_category_paper_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4c112",
   "metadata": {
    "id": "a6a4c112"
   },
   "source": [
    "### 데이터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00cc62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e00cc62",
    "outputId": "8da2f5bd-5463-4454-b6cc-0113cc82728a"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 0) 결과 폴더 생성: SS/results_S_ver2\n",
    "out_dir = Path.cwd() / \"results_S_ver4\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) 저장 대상 DF (df 사용)\n",
    "df_to_save = df.reset_index(drop=True)\n",
    "\n",
    "# 2) 파일 경로 (파일명 앞에 SS 사회과학 접두사 붙임)\n",
    "csv_path = out_dir / \"SS_kor_abstract_clean.csv\"\n",
    "pkl_path = out_dir / \"SS_kor_abstract_clean.pkl\"\n",
    "\n",
    "# 3) 저장\n",
    "df_to_save.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "df_to_save.to_pickle(pkl_path)\n",
    "\n",
    "# 4) 로그 & 간단 점검\n",
    "print(\"[저장 완료]\")\n",
    "print(\"Rows:\", len(df_to_save))\n",
    "print(\"CSV   →\", csv_path.resolve())\n",
    "print(\"Pickle →\", pkl_path.resolve())\n",
    "if \"발행년도\" in df_to_save.columns:\n",
    "    print(\"Year range:\", int(df_to_save[\"발행년도\"].min()), \"→\", int(df_to_save[\"발행년도\"].max()))\n",
    "if \"초록\" in df_to_save.columns:\n",
    "    print(\"초록 NaN 개수:\", int(df_to_save[\"초록\"].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b969fa",
   "metadata": {
    "id": "f6b969fa"
   },
   "source": [
    "### csv 미리 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72693492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "72693492",
    "outputId": "e0149373-b679-478d-82e0-4b073e325837"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = Path.cwd() / \"results_S_ver4\" / \"SS_kor_abstract_clean.csv\"\n",
    "\n",
    "# 1) 존재 확인\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"파일이 없습니다: {csv_path}\")\n",
    "\n",
    "# 2) 로드 (저장 시 UTF-8-SIG)\n",
    "df_loaded = pd.read_csv(csv_path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "# 3) 간단 미리보기\n",
    "print(\"[로드 완료]\")\n",
    "print(\"shape:\", df_loaded.shape)\n",
    "print(\"columns:\", list(df_loaded.columns))\n",
    "display(df_loaded.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e25b",
   "metadata": {
    "id": "8ee2e25b"
   },
   "source": [
    "## 2) 형태소 분석 및 패턴 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3139c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6be3139c",
    "outputId": "5a71dec8-0bab-4292-bfbd-3549c119b8d6"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 패턴 정의\n",
    "COMBINED_PATTERNS = [\n",
    "    # 1. '-하게' 부사형 복원 (정확하게, 복잡하게, 세련되게)\n",
    "    (\"XR\", \"XSV\", \"EC\"),\n",
    "    (\"XR\", \"XSA\", \"EC\"),\n",
    "    (\"NNG\", \"XSA\", \"EC\"),\n",
    "    (\"VA\", \"EC\"),\n",
    "\n",
    "    # 2. '-한' 관형형 복원 (화려한, 다양한, 적절한)\n",
    "    (\"XR\", \"XSA\", \"ETM\"),\n",
    "    (\"NNG\", \"XSA\", \"ETM\"),\n",
    "    (\"VA\", \"ETM\"),\n",
    "\n",
    "    # 3. 복합 형용사 관형형 (전형적인, 체계적인, 논리적인)\n",
    "    (\"NNG\", \"XSN\", \"VCP\", \"ETM\"), # 전형 + 적 + 이 + 은\n",
    "\n",
    "    # 4. '-적으로' 복원 (복합적으로, 이론적으로)\n",
    "    (\"NNG\", \"XSN\", \"JKB\"),\n",
    "\n",
    "    # 5. 핵심 복합 용언 '하다/되다' 동사/형용사 활용형 복원\n",
    "    (\"NNG\", \"XSV\", \"EF\"),        # 종결형: 주장하다\n",
    "    (\"NNG\", \"XSV\", \"ETM\"),       # 관형형: 주장하는\n",
    "    (\"NNG\", \"XSV\", \"EC\"),        # 연결형: 주장하고\n",
    "    (\"NNG\", \"XSV\", \"EP\", \"EF\"),  # 과거 종결형: 주장했다\n",
    "    (\"NNG\", \"XSV\", \"EP\", \"ETM\"), # 과거 관형형: 주장했던\n",
    "    (\"NNG\", \"XSV\", \"EP\", \"EC\"),  # 과거 연결형: 주장하였고\n",
    "    \n",
    "# 6. 일반 용언 활용형 복원\n",
    "    (\"VV\", \"ETM\"),       # 동사 어간 + 관형형 어미\n",
    "    (\"VA\", \"ETM\"),       # 형용사 어간 + 관형형 어미\n",
    "    (\"VV\", \"EP\", \"ETM\"), # 동사 과거형 + 관형형\n",
    "    (\"VA\", \"EP\", \"ETM\"), # 형용사 과거형 + 관형형\n",
    "    (\"VV\", \"EF\"),         # [추가] 동사 종결형 (사회과학 17위 / 인문학 15위)\n",
    "    (\"VV\", \"EP\", \"EF\"),   # [추가] 동사 과거 종결형 (사회과학 27위 / 인문학 28위)\n",
    "    (\"VV\", \"EC\"),         # [추가] 동사 연결형 (인문학 12위)\n",
    "\n",
    "    # 7. 보조 용언 활용형 복원\n",
    "    (\"VX\", \"ETM\"),        # 보조 용언 + 관형형 어미\n",
    "    (\"VX\", \"EP\", \"ETM\"),  # 보조 용언 과거 관형형\n",
    "    (\"VX\", \"EF\"),         # [추가] 보조용언 종결형 (인문학 24위)\n",
    "]\n",
    "\n",
    "# 결과 저장 경로 설정\n",
    "save_dir = Path('/home/work/KCI_LLM_Lab/Seul/results_S_ver4')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 산출물 경로 정의\n",
    "in_pkl = save_dir / \"SS_kor_abstract_clean.pkl\"                   # 전처리된 입력 데이터\n",
    "out_word_year_counts = save_dir / \"SS_word_year_matrix_counts.csv\" # 단어-연도 행렬\n",
    "out_doc_counts = save_dir / \"SS_doc_counts.csv\"                    # 연도별 문서 수\n",
    "tokenized_file = save_dir / '사회과학_tokenized_results.csv'        # 토큰화 결과\n",
    "\n",
    "# 전처리된 데이터 불러오기\n",
    "if not in_pkl.exists():\n",
    "    raise FileNotFoundError(f\"전처리된 파일이 없습니다: {in_pkl}\")\n",
    "    \n",
    "df = pd.read_pickle(in_pkl)\n",
    "print(f\"[OK] 데이터 로드 완료: {len(df)}개 문서\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"텍스트 전처리: 특수문자, 문장부호 등 제거\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "        \n",
    "    # 괄호와 그 내용 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    \n",
    "    # 특수문자 및 문장부호 제거\n",
    "    text = re.sub(r'[,.!?:;\\'\"\\/\\{\\}\\[\\]<>~`@#$%^&*=+|\\\\『』「」]', ' ', text)\n",
    "    \n",
    "    # 여러 개의 공백을 하나로 치환\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def initialize_kiwi():\n",
    "    \"\"\"각 프로세스에서 Kiwi 초기화 (kiwi v0.22.0 이상에서는 cong 모델 바로 사용 가능)\"\"\"\n",
    "    return Kiwi(model_type='cong')\n",
    "\n",
    "def analyze_words(text, kiwi=None):\n",
    "    \"\"\"텍스트의 각 어절에 대한 형태소 분석 수행\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # 텍스트 전처리\n",
    "    text = clean_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    if kiwi is None:\n",
    "        kiwi = initialize_kiwi()\n",
    "    \n",
    "    results = []\n",
    "    words = text.split()\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            # 각 어절에 대한 형태소 분석\n",
    "            morphs = kiwi.tokenize(word)\n",
    "            morph_sequence = [(m.form, m.tag) for m in morphs]\n",
    "            \n",
    "            # 해당 어절의 태그 시퀀스\n",
    "            tags = tuple(tag for form, tag in morph_sequence)  \n",
    "            \n",
    "            # 패턴 매칭 여부 확인\n",
    "            matches_pattern = any(\n",
    "                len(pattern) == len(tags) and all(p == t for p, t in zip(pattern, tags))\n",
    "                for pattern in COMBINED_PATTERNS\n",
    "            )\n",
    "            \n",
    "            # 모든 어절에 대해 형태소 분석 결과 저장\n",
    "            results.append({\n",
    "                'word': word,                  # 원본 어절\n",
    "                'morphs': morph_sequence,      # 형태소 분석 결과\n",
    "                'matches_pattern': matches_pattern  # 패턴 매칭 여부\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"형태소 분석 중 오류 발생: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def process_chunk(texts):\n",
    "    \"\"\"청크 단위로 토큰화 처리\"\"\"\n",
    "    kiwi = initialize_kiwi()\n",
    "    return [analyze_words(text, kiwi) for text in texts]\n",
    "\n",
    "def parallel_tokenize(texts, n_cores):\n",
    "    \"\"\"병렬 처리로 토큰화 수행\"\"\"\n",
    "    chunk_size = len(texts) // n_cores + 1\n",
    "    chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n",
    "    \n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_chunk, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=\"청크 처리 중\"\n",
    "        ))\n",
    "    \n",
    "    # 결과 합치기\n",
    "    return [token for chunk_result in results for token in chunk_result]\n",
    "\n",
    "# CPU 코어 수 확인 (전체 코어의 75% 사용)\n",
    "num_cores = max(1, int(mp.cpu_count() * 0.75))\n",
    "print(f\"\\n사용할 CPU 코어 수: {num_cores}\")\n",
    "\n",
    "# Kiwi 초기화 확인\n",
    "print(\"\\nKiwi 초기화 중...\")\n",
    "test_kiwi = Kiwi(model_type='cong')\n",
    "print(\"CoNG 모델 로드 완료\")\n",
    "\n",
    "# 병렬 토큰화 수행\n",
    "print(\"\\n초록 토큰화 시작...\")\n",
    "df['tokens'] = parallel_tokenize(df['초록'].tolist(), num_cores)\n",
    "\n",
    "# 토큰화 결과 저장\n",
    "print(\"\\n토큰화 결과 저장 중...\")\n",
    "df.to_csv(tokenized_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 연도별 단어 출현 빈도 계산 (COMBINED_PATTERNS에 매칭되는 어절만)\n",
    "print(\"\\n연도별 단어 빈도 계산 중...\")\n",
    "word_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "word_is_pattern = {}  # 단어가 패턴에 맞는지 여부 저장\n",
    "years = sorted(df['발행년도'].unique())\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"단어 빈도 계산\"):\n",
    "    year = row['발행년도']\n",
    "    tokens = row['tokens']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token['matches_pattern']:\n",
    "            # 패턴에 맞는 경우만 어절 단위로 저장\n",
    "            word = token['word']\n",
    "            word_year_counts[word][year] += 1\n",
    "            word_is_pattern[word] = True\n",
    "        # 패턴에 맞지 않는 경우는 제외 (결과에 포함하지 않음)\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "word_year_df = pd.DataFrame.from_dict(word_year_counts, orient='index')\n",
    "word_year_df = word_year_df.reindex(columns=years, fill_value=0)\n",
    "\n",
    "# 패턴 매칭 여부 열 추가\n",
    "word_year_df['matches_pattern'] = pd.Series(word_is_pattern)\n",
    "word_year_df.to_csv(out_word_year_counts, encoding='utf-8-sig')\n",
    "\n",
    "# 연도별 전체 문서 수 저장\n",
    "doc_counts = df['발행년도'].value_counts().sort_index()\n",
    "doc_counts.to_frame(name='n_docs').to_csv(out_doc_counts, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n✅ 완료:\")\n",
    "print(f\" - 토큰화 결과: {tokenized_file}\")\n",
    "print(f\" - 단어-연도 행렬: {out_word_year_counts}\")\n",
    "print(f\" - 연도별 문서 수: {out_doc_counts}\")\n",
    "print(f\" - 고유 단어 수: {len(word_year_counts)}\")\n",
    "print(f\" - 전체 문서 수: {len(df)}\")\n",
    "\n",
    "# 빈도수 상위 20개 단어와 그들의 패턴 매칭 여부 출력\n",
    "total_counts = word_year_df.drop('matches_pattern', axis=1).sum(axis=1)\n",
    "top_20 = total_counts.nlargest(20)\n",
    "print(\"\\n가장 빈번한 단어 20개:\")\n",
    "for word, count in top_20.items():\n",
    "    is_pattern = word_year_df.loc[word, 'matches_pattern']\n",
    "    pattern_str = \"패턴 매칭\" if is_pattern else \"개별 형태소\"\n",
    "    print(f\"{word}: {count}회 ({pattern_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e9120",
   "metadata": {
    "id": "375e9120"
   },
   "source": [
    "# 3) 연도별 빈도 계산 및 2024년 과잉 어휘 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887e5d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8887e5d6",
    "outputId": "327f4795-53bd-467e-995f-b20c00f24589"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------\n",
    "# 0) 경로\n",
    "# ------------------------------\n",
    "\n",
    "base_dir = Path(\"results_S_ver4\")\n",
    "counts_path = base_dir / \"SS_word_year_matrix_counts.csv\"\n",
    "docs_path   = base_dir / \"SS_doc_counts.csv\"\n",
    "\n",
    "out_full    = base_dir / \"SS_excess_words_2024.csv\"          # 2024년 분석 대상 모든 단어의 결과\n",
    "out_filtered= base_dir / \"SS_excess_words_2024_filtered.csv\" # 임계값 필터하여 과잉어휘만 모은 결과\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1) 데이터 로드\n",
    "# ------------------------------\n",
    "counts = pd.read_csv(counts_path, index_col=0)   # 행 = terms, 열 = years\n",
    "doc_counts = pd.read_csv(docs_path, index_col=0)[\"n_docs\"]  # 인덱스 = years, 밸류 = n_docs\n",
    "\n",
    "# 분석에 쓸 연도들\n",
    "YEARS = [2021, 2022, 2023, 2024]\n",
    "missing = [y for y in YEARS if str(y) not in counts.columns or y not in doc_counts.index]\n",
    "if missing:\n",
    "    raise ValueError(f\"필요 연도 누락: {missing}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2) 단어의 연도별 등장비율 p 계산,  p = (a+1)/(b+1)\n",
    "# ------------------------------\n",
    "p_s = {}\n",
    "for y in YEARS:\n",
    "    a = counts[str(y)].astype(float)\n",
    "    b = float(doc_counts.loc[y])\n",
    "    p_s[y] = (a + 1.0) / (b + 1.0) #a는 그 연도에 해당 단어가 등장한 문서 수, b는 그 연도 전체 문서 수\n",
    "    # 즉, 그 연도 전체 문서 중 몇 %에서 해당 단어가 쓰였는가. +1은 0으로 나누는 걸 방지함\n",
    "\n",
    "p_df = pd.DataFrame(p_s)  # p_df는 index = 단어, columns = [2021, 2022, 2023, 2024] 형태의 “연도별 등장비율 행렬”\n",
    "p_df.index.name = \"term\" # 각 단어(term)가 2021–2024년 각 연도 초록 중 몇 %에서 등장했는가를 담은 비율 행렬\n",
    "\n",
    "# ------------------------------\n",
    "# 3) 분석 대상 단어 선택 (빈도 ≥ 1e-4 in 2024 AND 2023)\n",
    "# 빈도 ≥ 1e-4는 희귀한 잡음을 제거하려는 목적이므로 단어선정의 안정성을 위해 2023년도도 포함함\n",
    "# (아래의 예상치 계산에서는 2023년도는 사용하지 않는다는 점이 다름)\n",
    "# ------------------------------\n",
    "min_p = 1e-4\n",
    "mask_keep = (p_df[2024] >= min_p) & (p_df[2023] >= min_p)\n",
    "p_df = p_df.loc[mask_keep].copy()\n",
    "\n",
    "# ------------------------------\n",
    "# 4) q(2024 예상치) 계산 - 외삽 규칙\n",
    "#     q = p2022 + 2*max(p2022 - p2021, 0)   # (증가 추세면 2배 증가, 감소 추세면 현상 유지)\n",
    "# -> 하락한 단어는 증가하지 않는다는 보수적 가정, 즉, q >= p2022\n",
    "# -> 상승한 단어는 증가 추세를 반영해서 2배 반영, q를 높게 잡아 δ = p − q가 부풀려지지 않도록(보수적 하한) 설계\n",
    "# ------------------------------\n",
    "p21 = p_df[2021].values\n",
    "p22 = p_df[2022].values\n",
    "p24 = p_df[2024].values\n",
    "\n",
    "trend = np.maximum(p22 - p21, 0.0)\n",
    "q = p22 + 2.0 * trend\n",
    "q = np.maximum(q, p22)  # 보수적 하한\n",
    "\n",
    "# ------------------------------\n",
    "# 5) δ, ratio, log10 r 계산\n",
    "# ------------------------------\n",
    "eps = 1e-12\n",
    "delta = p24 - q\n",
    "ratio = p24 / np.maximum(q, eps)\n",
    "log10r = np.log10(np.maximum(ratio, eps))\n",
    "\n",
    "# ------------------------------\n",
    "# 6) 결과 테이블 구성\n",
    "# ------------------------------\n",
    "out = pd.DataFrame({\n",
    "    \"p2021\": p_df[2021],\n",
    "    \"p2022\": p_df[2022],\n",
    "    \"p2023\": p_df[2023],\n",
    "    \"p2024\": p_df[2024],\n",
    "    \"q2024\": q,\n",
    "    \"delta\": delta,\n",
    "    \"ratio\": ratio,\n",
    "    \"log10_ratio\": log10r,\n",
    "})\n",
    "\n",
    "# 정렬: δ와 ratio 모두 큰 순으로 참고하기 좋도록\n",
    "out_sorted = out.sort_values([\"delta\", \"ratio\"], ascending=[False, False])\n",
    "\n",
    "# 전체 저장\n",
    "out_sorted.to_csv(out_full, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7) 임계값 필터 (수정 버전)\n",
    "#    - δ > 0.01  또는  log10 r > 0.1 (고정 컷)\n",
    "#    - + 빈도 필터: p2024, p2023 ≥ 1e-4\n",
    "# ------------------------------\n",
    "gap_thr = 0.01          # 절대적 빈도 차이 기준; 0.01 = 1%p 이상의 과잉만 채택\n",
    "logr_thr = 0.1          #  log10(r) 고정 컷; 너무 많은 단어가 통과하지 않도록 보수적으로 설정\n",
    "min_freq = 1e-4         # 분석 연도(Y)와 직전 연도(Y-1) 모두 p ≥ 1e-4인 단어만 대상\n",
    "eps = 1e-12             # 수치 안정성용 아주 작은 값(0 나눗셈, log(0) 방지)\n",
    "\n",
    "# 빈도 필터 선적용 (Y와 Y-1에서 p ≥ 1e-4)\n",
    "# 극저빈도(희귀) 단어는 작은 변동에도 δ, r이 크게 출렁이며 log 기준을 쉽게 통과하므로\n",
    "# 먼저 제거해 과검출을 줄임\n",
    "mask_freq = (out_sorted[\"p2024\"] >= min_freq) & (out_sorted[\"p2023\"] >= min_freq)\n",
    "cand = out_sorted[mask_freq]\n",
    "\n",
    "# --- 임계식 적용 ---\n",
    "# 원논문과 다른 점은 log 기준을 고정값(0.1)로 보수화하여 과검출을 억제함.\n",
    "#  원 논문의 log 컷은 대략 0.075(≈19% 증가) 이며 이 분석의 경우 0.1(≈26% 증가)로 상향 조정해서 과검출(거짓양성)을 줄임.\n",
    "filtered = cand[\n",
    "    (cand[\"delta\"] > gap_thr) |\n",
    "    (cand[\"log10_ratio\"] > logr_thr)\n",
    "]\n",
    "\n",
    "# 임계값 통과 단어 수 (필터 전/후 모두 표기)\n",
    "print(f\"임계값 통과 단어 수: {len(filtered)} / {len(cand)} (원본 {len(out_sorted)})\")\n",
    "\n",
    "# 저장\n",
    "filtered.to_csv(out_filtered, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ 완료:\")\n",
    "print(\" - 전체 표  :\", out_full)\n",
    "print(\" - 임계값 표:\", out_filtered)  # 최종 Excess words 리스트\n",
    "print(\" - 대상 단어 수(원본):\", len(out_sorted))\n",
    "print(\" - 빈도 필터 후 대상:\", len(cand))\n",
    "print(\" - 임계 통과 수     :\", len(filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a09483",
   "metadata": {
    "id": "f2a09483"
   },
   "source": [
    "# 4)  과잉어휘 목록 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3b630",
   "metadata": {},
   "source": [
    "2025.12.25 Ratio 나열시 최소 문서수 30이상 기준으로 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f116ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta 및 ratio 기준 상위 과잉어휘 100 + 100 병합 --> 파일 생성\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"results_S_ver4\")\n",
    "filtered = pd.read_csv(base / \"SS_excess_words_2024_filtered.csv\")\n",
    "\n",
    "# 인덱스 복원/표준화: 'term' → 'word' 로 통일\n",
    "filtered = filtered.reset_index(drop=True)\n",
    "if \"word\" not in filtered.columns:\n",
    "    if \"term\" in filtered.columns:\n",
    "        filtered = filtered.rename(columns={\"term\": \"word\"})\n",
    "    elif \"index\" in filtered.columns:\n",
    "        filtered = filtered.rename(columns={\"index\": \"word\"})\n",
    "    elif \"Unnamed: 0\" in filtered.columns:\n",
    "        filtered = filtered.rename(columns={\"Unnamed: 0\": \"word\"})\n",
    "    else:\n",
    "        filtered = filtered.reset_index().rename(columns={\"index\": \"word\"})\n",
    "\n",
    "# 숫자 컬럼 보정 (안전)\n",
    "for col in (\"delta\", \"ratio\", \"p2024\"):\n",
    "    if col in filtered.columns:\n",
    "        filtered[col] = pd.to_numeric(filtered[col], errors=\"coerce\")\n",
    "\n",
    "# -------------------------\n",
    "# 2024 실제 문서 수 기반 df 계산\n",
    "# -------------------------\n",
    "N_DOCS_2024 = 25198\n",
    "filtered[\"df2024\"] = (filtered[\"p2024\"] * N_DOCS_2024).round().astype(\"Int64\")\n",
    "\n",
    "# -------------------------\n",
    "# ratio 필터 기준: 최소 문서수\n",
    "# -------------------------\n",
    "MIN_DOCS = 30  # 수정 해 가면서 볼 수 있음(df=10·20·30을 비교한 민감도 점검 결과, \n",
    "# df=30은 희귀 단어들이 ratio 상위에 포함되는 현상을 억제하면서도, \n",
    "# 반복적으로 사용된 표현을 보존하는 최소 기준으로 적절해 보임)\n",
    "ratio_pool = filtered[filtered[\"df2024\"] >= MIN_DOCS].copy()\n",
    "\n",
    "print(f\"✅ ratio 후보군: {len(ratio_pool)} / {len(filtered)} (df2024 >= {MIN_DOCS})\")\n",
    "\n",
    "# 전체 데이터 기준 delta 랭킹만 계산 (참고용)\n",
    "tmp = filtered.copy()\n",
    "tmp[\"delta_rank\"] = tmp[\"delta\"].rank(ascending=False, method=\"min\")\n",
    "\n",
    "# Top100 추출\n",
    "top_delta = filtered.nlargest(100, \"delta\")\n",
    "top_ratio = ratio_pool.nlargest(100, \"ratio\")  # <-- df 필터 통과한 후보군에서만 추출\n",
    "\n",
    "# 합집합 + 중복 제거\n",
    "combined = pd.concat([top_delta, top_ratio], ignore_index=True).drop_duplicates(\"word\")\n",
    "\n",
    "# delta_rank만 병합 (전체 데이터 기준 순위 유지)\n",
    "combined = combined.merge(tmp[[\"word\", \"delta_rank\"]], on=\"word\", how=\"left\")\n",
    "\n",
    "# ratio_rank는 combined 데이터 내에서 새로 계산 (1~100 순위)\n",
    "combined[\"ratio_rank\"] = combined[\"ratio\"].rank(ascending=False, method=\"min\")\n",
    "\n",
    "# 라벨 컬럼 없으면 생성\n",
    "if \"manual_label\" not in combined.columns:\n",
    "    combined[\"manual_label\"] = \"\"   # 이후 수동 라벨링: style/content 등\n",
    "\n",
    "# 출력 파일명\n",
    "out_csv   = base / \"SS_excess_words_top_combined_for_labeling_final.csv\"\n",
    "out_excel = base / \"SS_excess_words_top_combined_for_labeling_final.xlsx\"\n",
    "\n",
    "combined.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "combined.to_excel(out_excel, index=False)\n",
    "\n",
    "print(f\"✅ 총 {len(combined)}개 단어가 추출되어 저장되었습니다. → {out_csv}\")\n",
    "print(f\"✅ 엑셀 파일로도 저장되었습니다. → {out_excel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db0e5c",
   "metadata": {},
   "source": [
    "이후 한글 초록 분석의 시각화는 영어 분석 코드(English_analysis.ipynb)로 입력 데이터만 변경하여 진행하였음"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
